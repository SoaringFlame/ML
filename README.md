# ML
```markdown
# Machine Learning Educational Project / –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é

## üìö Table of Contents / –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
- [English Version](#english-version)
  - [Project Description](#project-description)
  - [Technologies Used](#technologies-used)
  - [Features](#features)
  - [Notebooks Overview](#notebooks-overview)
  - [Usage Examples](#usage-examples)
- [–†—É—Å—Å–∫–∞—è –í–µ—Ä—Å–∏—è](#russian-version)
  - [–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞](#–æ–ø–∏—Å–∞–Ω–∏–µ-–ø—Ä–æ–µ–∫—Ç–∞)
  - [–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏](#–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏)
  - [–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏](#–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏)
  - [–û–±–∑–æ—Ä –Ω–æ—É—Ç–±—É–∫–æ–≤](#–æ–±–∑–æ—Ä-–Ω–æ—É—Ç–±—É–∫–æ–≤)
  - [–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è](#–ø—Ä–∏–º–µ—Ä—ã-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)

---

```markdown
# Machine Learning Educational Project

## üìä Project Overview

This is a comprehensive educational project covering fundamental machine learning algorithms and their implementations. The project includes Jupyter notebooks with detailed explanations and practical examples of linear regression, logistic regression, perceptrons, and neural networks.

## üéØ Project Description

This collection of Jupyter notebooks serves as a complete learning resource for understanding core machine learning concepts:

- **Linear Regression** - Theory, implementation, and application to real datasets
- **Logistic Regression** - Classification problems and marketing campaign prediction
- **Perceptron Algorithm** - Basic neural network implementation from scratch
- **Neural Networks** - Implementation with different activation functions and loss functions

The project is designed for:
- Students learning machine learning fundamentals
- Data science practitioners seeking hands-on examples
- Developers implementing ML algorithms from scratch

## üìö Table of Contents

- [Technologies Used](#-technologies-used)
- [Installation & Setup](#-installation--setup)
- [Project Structure](#-project-structure)
- [Features](#-features)
- [Notebooks Overview](#-notebooks-overview)
- [Usage Examples](#-usage-examples)
- [License](#-license)
- [FAQ](#-faq)
- [Author](#-author)

## üõ† Technologies Used

**Core Technologies:**
- Python 3.8+
- Jupyter Notebook

**Data Science Libraries:**
- NumPy - Numerical computations
- Pandas - Data manipulation and analysis
- Matplotlib - Data visualization
- Seaborn - Statistical data visualization
- scikit-learn - Machine learning algorithms and metrics

**Machine Learning Algorithms:**
- Linear Regression
- Ridge & Lasso Regression
- Logistic Regression
- Support Vector Machines (SVM)
- Perceptron
- Neural Networks with various activation functions

## üíª Installation & Setup

### Prerequisites
- Python 3.7+
- Jupyter Notebook/JupyterLab

### Installation Steps

1. **Clone the repository**
```bash
git clone <repository-url>
cd machine-learning-educational-project
```

2. **Create virtual environment (recommended)**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install jupyter numpy pandas matplotlib scikit-learn seaborn
```

4. **Launch Jupyter Notebook**
```bash
jupyter notebook
```

5. **Open and run notebooks sequentially**



## ‚ú® Features

### üîß Core Features
- **Comprehensive Theory** - Mathematical foundations with detailed explanations
- **From-Scratch Implementations** - Custom implementations of ML algorithms
- **Real Dataset Applications** - Practical examples with real-world data
- **Visualization** - Extensive plotting and data visualization
- **Model Evaluation** - Multiple metrics and performance analysis

### üìà Educational Features
- Step-by-step mathematical derivations
- Interactive code examples with explanations
- Comparative analysis of different algorithms
- Hands-on exercises and testing
- Problem-solving approaches for common ML challenges

## üìì Notebooks Overview

### 1. Linear Regression
- Theoretical foundations of linear regression
- Synthetic data generation and visualization
- Model training and evaluation (MSE, MAE)
- Real-world application with Boston housing dataset
- Ridge and Lasso regularization
- Feature engineering demonstrations

### 2. Logistic Regression
- Binary classification theory
- Marketing campaign prediction case study
- Data preprocessing and one-hot encoding
- Model evaluation with accuracy, F1-score, and ROC-AUC
- Comparison with SVM

### 3. Perceptron Implementation
- Perceptron algorithm from scratch
- Custom class implementation with forward/backward passes
- Testing on synthetic and real datasets
- Comparison with scikit-learn's Perceptron
- Gender recognition from voice data

### 4. Neural Networks
- Neural network implementation with sigmoid activation
- Gradient descent optimization
- LogLoss function implementation
- Comparison of different loss functions
- Vanishing gradients problem analysis

## üöÄ Usage Examples

### Linear Regression Example
```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

print('Test MSE: ', mean_squared_error(y_test, predictions))
```

### Logistic Regression Example
```python
from sklearn.linear_model import LogisticRegression

model_logistic = LogisticRegression()
model_logistic.fit(X_train, y_train)
y_predicted = model_logistic.predict_proba(X_test)[:, 1]
```

### Custom Perceptron Example
```python
perceptron = Perceptron()
losses = perceptron.fit(X, y, num_epochs=300)
predictions = perceptron.forward_pass(X_test)
```

### Neural Network with LogLoss
```python
neuron = Neuron()
J_values = neuron.fit(X, y)
predictions = neuron.forward_pass(X_test)
```

---

# Russian Version

## üéØ –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞

–≠—Ç–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å–ª—É–∂–∏—Ç –ø–æ–ª–Ω—ã–º —É—á–µ–±–Ω—ã–º —Ä–µ—Å—É—Ä—Å–æ–º –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:

- **–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è** - –¢–µ–æ—Ä–∏—è, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ —Ä–µ–∞–ª—å–Ω—ã–º –Ω–∞–±–æ—Ä–∞–º –¥–∞–Ω–Ω—ã—Ö
- **–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è** - –ó–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π
- **–ê–ª–≥–æ—Ä–∏—Ç–º –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞** - –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å –Ω—É–ª—è
- **–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏** - –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –ø–æ—Ç–µ—Ä—å

–ü—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è:
- –°—Ç—É–¥–µ–Ω—Ç–æ–≤, –∏–∑—É—á–∞—é—â–∏—Ö –æ—Å–Ω–æ–≤—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- –ü—Ä–∞–∫—Ç–∏–∫–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ data science, –∏—â—É—â–∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã
- –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤, —Ä–µ–∞–ª–∏–∑—É—é—â–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ã ML —Å –Ω—É–ª—è

## üõ† –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

**–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:**
- Python 3.8+
- Jupyter Notebook

**–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö:**
- NumPy - –ß–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
- Pandas - –ú–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
- Matplotlib - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- Seaborn - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- scikit-learn - –ê–ª–≥–æ—Ä–∏—Ç–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏

**–ê–ª–≥–æ—Ä–∏—Ç–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:**
- –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
- Ridge & Lasso —Ä–µ–≥—Ä–µ—Å—Å–∏—è
- –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
- –ú–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (SVM)
- –ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω
- –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏

## ‚ú® –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### üîß –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- **–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Ç–µ–æ—Ä–∏—è** - –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
- **–†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –Ω—É–ª—è** - –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ ML
- **–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö** - –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è** - –û–±—à–∏—Ä–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- **–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π** - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### üìà –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- –ü–æ—à–∞–≥–æ–≤—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã
- –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
- –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- –ü–æ–¥—Ö–æ–¥—ã –∫ —Ä–µ—à–µ–Ω–∏—é —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º ML

## üìì –û–±–∑–æ—Ä –Ω–æ—É—Ç–±—É–∫–æ–≤

### 1. –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
- –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
- –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –æ—Ü–µ–Ω–∫–∞ (MSE, MAE)
- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö –æ —Ü–µ–Ω–∞—Ö –Ω–∞ –∂–∏–ª—å–µ –≤ –ë–æ—Å—Ç–æ–Ω–µ
- –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è Ridge –∏ Lasso
- –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

### 2. –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
- –¢–µ–æ—Ä–∏—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- –ö–µ–π—Å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–π –∫–∞–º–ø–∞–Ω–∏–∏
- –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
- –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é, F1-score –∏ ROC-AUC
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å SVM

### 3. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞
- –ê–ª–≥–æ—Ä–∏—Ç–º –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞ —Å –Ω—É–ª—è
- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞ —Å –ø—Ä—è–º—ã–º/–æ–±—Ä–∞—Ç–Ω—ã–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ–º
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–æ–º –∏–∑ scikit-learn
- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ø–æ–ª–∞ –ø–æ –≥–æ–ª–æ—Å–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º

### 4. –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏
- –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å —Å–∏–≥–º–æ–∏–¥–∞–ª—å–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º
- –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å LogLoss
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å
- –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º—ã –∑–∞—Ç—É—Ö–∞—é—â–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤

## üöÄ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

print('Test MSE: ', mean_squared_error(y_test, predictions))
```

### –ü—Ä–∏–º–µ—Ä –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
```python
from sklearn.linear_model import LogisticRegression

model_logistic = LogisticRegression()
model_logistic.fit(X_train, y_train)
y_predicted = model_logistic.predict_proba(X_test)[:, 1]
```

### –ü—Ä–∏–º–µ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞
```python
perceptron = Perceptron()
losses = perceptron.fit(X, y, num_epochs=300)
predictions = perceptron.forward_pass(X_test)
```

### –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å LogLoss
```python
neuron = Neuron()
J_values = neuron.fit(X, y)
predictions = neuron.forward_pass(X_test)
```