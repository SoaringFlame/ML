# ML

# Machine Learning Educational Project / –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é

<div align="center">

![Machine Learning](https://img.shields.io/badge/Machine-Learning-blue)
![Python](https://img.shields.io/badge/Python-3.8%2B-green)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

**–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –ø–æ –æ—Å–Ω–æ–≤–∞–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è**

</div>

## üìö Table of Contents / –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

### English Version
- [üåü Project Overview](#-project-overview)
- [üéØ Project Description](#-project-description)
- [üõ† Technologies Used](#-technologies-used)
- [üöÄ Quick Start](#-quick-start)
- [üìÅ Project Structure](#-project-structure)
- [‚ú® Features](#-features)
- [üìì Notebooks Overview](#-notebooks-overview)
- [üí° Usage Examples](#-usage-examples)
- [‚ùì FAQ](#-faq)

### –†—É—Å—Å–∫–∞—è –í–µ—Ä—Å–∏—è
- [üåü –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞](#-–æ–±–∑–æ—Ä-–ø—Ä–æ–µ–∫—Ç–∞)
- [üéØ –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞](#-–æ–ø–∏—Å–∞–Ω–∏–µ-–ø—Ä–æ–µ–∫—Ç–∞)
- [üõ† –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏](#-–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏)
- [üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç](#-–±—ã—Å—Ç—Ä—ã–π-—Å—Ç–∞—Ä—Ç)
- [üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞](#-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞-–ø—Ä–æ–µ–∫—Ç–∞)
- [‚ú® –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏](#-–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏)
- [üìì –û–±–∑–æ—Ä –Ω–æ—É—Ç–±—É–∫–æ–≤](#-–æ–±–∑–æ—Ä-–Ω–æ—É—Ç–±—É–∫–æ–≤)
- [üí° –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è](#-–ø—Ä–∏–º–µ—Ä—ã-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
- [‚ùì –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã](#-—á–∞—Å—Ç–æ-–∑–∞–¥–∞–≤–∞–µ–º—ã–µ-–≤–æ–ø—Ä–æ—Å—ã)

---

# English Version

## üåü Project Overview

This is a comprehensive educational project covering fundamental machine learning algorithms and their implementations. The project includes Jupyter notebooks with detailed explanations and practical examples of linear regression, logistic regression, perceptrons, and neural networks.

<div align="center">

```mermaid
graph TD
    A[ML Educational Project] --> B[Linear Regression]
    A --> C[Logistic Regression]
    A --> D[Perceptron]
    A --> E[Neural Networks]
    
    B --> B1[Theory]
    B --> B2[Implementation]
    B --> B3[Real Data]
    
    C --> C1[Classification]
    C --> C2[Marketing Case]
    C --> C3[Evaluation]
    
    D --> D1[From Scratch]
    D --> D2[Custom Class]
    D --> D3[Comparison]
    
    E --> E1[Activation Functions]
    E --> E2[Loss Functions]
    E --> E3[Optimization]

## üéØ Project Description

This collection of Jupyter notebooks serves as a complete learning resource for understanding core machine learning concepts:

- **üìà Linear Regression** - Theory, implementation, and application to real datasets
- **üéØ Logistic Regression** - Classification problems and marketing campaign prediction
- **üß† Perceptron Algorithm** - Basic neural network implementation from scratch
- **üîÆ Neural Networks** - Implementation with different activation functions and loss functions

### üéì Target Audience
- **Students** learning machine learning fundamentals
- **Data Science Practitioners** seeking hands-on examples
- **Developers** implementing ML algorithms from scratch
- **Researchers** exploring basic ML concepts

## üõ† Technologies Used

### üîß Core Technologies
<div align="center">

| Technology | Version | Purpose |
|------------|---------|---------|
| Python | 3.8+ | Core programming language |
| Jupyter Notebook | Latest | Interactive development environment |
| NumPy | 1.21+ | Numerical computations |
| Pandas | 1.3+ | Data manipulation and analysis |
| Matplotlib | 3.4+ | Data visualization |
| Seaborn | 0.11+ | Statistical data visualization |
| scikit-learn | 1.0+ | Machine learning algorithms |

</div>

### üìä Machine Learning Algorithms
- **Linear Models**: Linear Regression, Ridge, Lasso
- **Classification**: Logistic Regression, SVM, Perceptron
- **Neural Networks**: Custom implementations with various activations
- **Evaluation Metrics**: MSE, MAE, Accuracy, F1-score, ROC-AUC

## üöÄ Quick Start

### ‚ö° Prerequisites
- Python 3.7 or higher
- pip package manager
- Git (for cloning repository)

### üì• Installation Steps

1. **Clone the repository**
```bash
git clone https://github.com/your-username/machine-learning-project.git
cd machine-learning-project
```

2. **Create and activate virtual environment**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/MacOS
python3 -m venv venv
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Launch Jupyter Notebook**
```bash
jupyter notebook
```

5. **Explore notebooks in order:**
   - `Linear_Regression.ipynb`
   - `Logistic_Regression.ipynb` 
   - `practice_perceptron.ipynb`
   - `practice_neuron.ipynb`
   - `practice_neuron_logloss.ipynb`

### üß™ Testing Installation
```python
# Test your installation
import numpy as np
import pandas as pd
import sklearn
print("All packages installed successfully! üéâ")
```

## üìÅ Project Structure

```
machine-learning-educational-project/
‚îÇ
‚îú‚îÄ‚îÄ üìì Notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ Linear_Regression.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Logistic_Regression.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ practice_perceptron.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ practice_neuron.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ practice_neuron_logloss.ipynb
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/
‚îÇ   ‚îú‚îÄ‚îÄ apples_pears.csv
‚îÇ   ‚îú‚îÄ‚îÄ bank.csv
‚îÇ   ‚îú‚îÄ‚îÄ bank-additional-full.csv
‚îÇ   ‚îî‚îÄ‚îÄ voice.csv
‚îÇ
‚îú‚îÄ‚îÄ üìÑ requirements.txt
‚îú‚îÄ‚îÄ üìú LICENSE
‚îî‚îÄ‚îÄ üìñ README.md
```

## ‚ú® Features

### üîß Core Features
| Feature | Description | Icon |
|---------|-------------|------|
| **Comprehensive Theory** | Mathematical foundations with detailed explanations | üìö |
| **From-Scratch Implementations** | Custom implementations of ML algorithms | üõ†Ô∏è |
| **Real Dataset Applications** | Practical examples with real-world data | üåç |
| **Advanced Visualization** | Extensive plotting and data visualization | üìä |
| **Model Evaluation** | Multiple metrics and performance analysis | ‚úÖ |

### üìà Educational Features
- **Step-by-step mathematical derivations** with LaTeX formulas
- **Interactive code examples** with detailed explanations
- **Comparative analysis** of different algorithms
- **Hands-on exercises** and testing scenarios
- **Problem-solving approaches** for common ML challenges
- **Visual learning aids** and diagrams

## üìì Notebooks Overview

### 1. üìà Linear Regression
<div align="center">

| Topic | Description | Skills |
|-------|-------------|--------|
| **Theory** | Mathematical foundations | Mathematics |
| **Synthetic Data** | Data generation & visualization | NumPy, Matplotlib |
| **Model Training** | Implementation & evaluation | scikit-learn |
| **Real Application** | Boston housing dataset | Pandas, Data Analysis |
| **Regularization** | Ridge & Lasso techniques | Model Optimization |
</div>

### 2. üéØ Logistic Regression
- **Binary classification theory** with probability interpretation
- **Marketing campaign prediction** case study
- **Data preprocessing** and one-hot encoding techniques
- **Model evaluation** with accuracy, F1-score, and ROC-AUC
- **Comparison with SVM** for classification tasks

### 3. üß† Perceptron Implementation
- **Perceptron algorithm** from first principles
- **Custom class implementation** with forward/backward passes
- **Testing on diverse datasets** including synthetic and real data
- **Performance comparison** with scikit-learn's Perceptron
- **Gender recognition** from voice characteristics

### 4. üîÆ Neural Networks
- **Neural network implementation** with sigmoid activation
- **Gradient descent optimization** techniques
- **LogLoss function implementation** and analysis
- **Comparison of different loss functions** (MSE vs LogLoss)
- **Vanishing gradients problem** and solutions

## üí° Usage Examples

### üìà Linear Regression Example
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Create and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate performance
mse = mean_squared_error(y_test, predictions)
print(f'Test MSE: {mse:.4f}')

# Visualize results
plt.figure(figsize=(10, 6))
plt.scatter(y_test, predictions, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Linear Regression: Actual vs Predicted')
plt.show()
```

### üéØ Logistic Regression Example
```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

# Initialize and train model
model_logistic = LogisticRegression(random_state=42)
model_logistic.fit(X_train, y_train)

# Get probability predictions
y_pred_proba = model_logistic.predict_proba(X_test)[:, 1]
y_pred = model_logistic.predict(X_test)

# Comprehensive evaluation
print("Classification Report:")
print(classification_report(y_test, y_pred))
print(f"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")
```

### üß† Custom Perceptron Example
```python
from sklearn.metrics import accuracy_score

# Initialize custom perceptron
perceptron = Perceptron()
losses = perceptron.fit(X_train, y_train, num_epochs=300)

# Make predictions
predictions = perceptron.forward_pass(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, predictions)
print(f'Perceptron Accuracy: {accuracy:.4f}')

# Plot training progress
plt.plot(losses)
plt.title('Perceptron Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()
```

### üîÆ Neural Network with LogLoss
```python
# Initialize neural network with LogLoss
neuron = Neuron()
J_values = neuron.fit(X, y, num_epochs=5000)

# Generate predictions
predictions = neuron.forward_pass(X_test)
binary_predictions = (predictions > 0.5).astype(int)

# Evaluate performance
accuracy = accuracy_score(y_test, binary_predictions)
print(f'Neural Network Accuracy: {accuracy:.4f}')

# Visualize learning curve
plt.figure(figsize=(10, 6))
plt.plot(J_values)
plt.title('Neural Network Training (LogLoss)')
plt.xlabel('Iteration')
plt.ylabel('LogLoss')
plt.grid(True)
plt.show()
```

---

# –†—É—Å—Å–∫–∞—è –í–µ—Ä—Å–∏—è

## üåü –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞

–≠—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏. –ü—Ä–æ–µ–∫—Ç –≤–∫–ª—é—á–∞–µ—Ç Jupyter notebooks —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–æ–≤ –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.

## üéØ –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞

–≠—Ç–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è Jupyter notebooks —Å–ª—É–∂–∏—Ç –ø–æ–ª–Ω—ã–º —É—á–µ–±–Ω—ã–º —Ä–µ—Å—É—Ä—Å–æ–º –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:

- **üìà –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è** - –¢–µ–æ—Ä–∏—è, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ —Ä–µ–∞–ª—å–Ω—ã–º –Ω–∞–±–æ—Ä–∞–º –¥–∞–Ω–Ω—ã—Ö
- **üéØ –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è** - –ó–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π
- **üß† –ê–ª–≥–æ—Ä–∏—Ç–º –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞** - –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å –Ω—É–ª—è
- **üîÆ –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏** - –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –ø–æ—Ç–µ—Ä—å

### üéì –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è
- **–°—Ç—É–¥–µ–Ω—Ç—ã**, –∏–∑—É—á–∞—é—â–∏–µ –æ—Å–Ω–æ–≤—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- **–ü—Ä–∞–∫—Ç–∏–∫–∏ Data Science**, –∏—â—É—â–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã
- **–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏**, —Ä–µ–∞–ª–∏–∑—É—é—â–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã ML —Å –Ω—É–ª—è
- **–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏**, –∏–∑—É—á–∞—é—â–∏–µ –±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ ML

## üõ† –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

### üîß –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
<div align="center">

| –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è | –í–µ—Ä—Å–∏—è | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |
|------------|---------|------------|
| Python | 3.8+ | –û—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è |
| Jupyter Notebook | –ü–æ—Å–ª–µ–¥–Ω—è—è | –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å—Ä–µ–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ |
| NumPy | 1.21+ | –ß–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è |
| Pandas | 1.3+ | –ú–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö |
| Matplotlib | 3.4+ | –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö |
| Seaborn | 0.11+ | –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è |
| scikit-learn | 1.0+ | –ê–ª–≥–æ—Ä–∏—Ç–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è |

</div>

### üìä –ê–ª–≥–æ—Ä–∏—Ç–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- **–õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏**: –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è, Ridge, Lasso
- **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è, SVM, –ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω
- **–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏**: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º–∏
- **–ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏**: MSE, MAE, Accuracy, F1-score, ROC-AUC

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### ‚ö° –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- Python 3.7 –∏–ª–∏ –≤—ã—à–µ
- –ú–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–∫–µ—Ç–æ–≤ pip
- Git (–¥–ª—è –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è)

### üì• –®–∞–≥–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

1. **–ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π**
```bash
git clone https://github.com/your-username/machine-learning-project.git
cd machine-learning-project
```

2. **–°–æ–∑–¥–∞–π—Ç–µ –∏ –∞–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/MacOS
python3 -m venv venv
source venv/bin/activate
```

3. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**
```bash
pip install -r requirements.txt
```

4. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ Jupyter Notebook**
```bash
jupyter notebook
```

5. **–ò–∑—É—á–∞–π—Ç–µ notebooks –ø–æ –ø–æ—Ä—è–¥–∫—É:**
   - `Linear_Regression.ipynb`
   - `Logistic_Regression.ipynb`
   - `practice_perceptron.ipynb`
   - `practice_neuron.ipynb`
   - `practice_neuron_logloss.ipynb`

### üß™ –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
```python
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É
import numpy as np
import pandas as pd
import sklearn
print("–í—Å–µ –ø–∞–∫–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã! üéâ")
```

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π-–ø—Ä–æ–µ–∫—Ç-ml/
‚îÇ
‚îú‚îÄ‚îÄ üìì –ù–æ—É—Ç–±—É–∫–∏/
‚îÇ   ‚îú‚îÄ‚îÄ Linear_Regression.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Logistic_Regression.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ practice_perceptron.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ practice_neuron.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ practice_neuron_logloss.ipynb
‚îÇ
‚îú‚îÄ‚îÄ üìÅ –¥–∞–Ω–Ω—ã–µ/
‚îÇ   ‚îú‚îÄ‚îÄ apples_pears.csv
‚îÇ   ‚îú‚îÄ‚îÄ bank.csv
‚îÇ   ‚îú‚îÄ‚îÄ bank-additional-full.csv
‚îÇ   ‚îî‚îÄ‚îÄ voice.csv
‚îÇ
‚îú‚îÄ‚îÄ üìÑ requirements.txt
‚îú‚îÄ‚îÄ üìú LICENSE
‚îî‚îÄ‚îÄ üìñ README.md
```

## ‚ú® –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### üîß –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
| –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å | –û–ø–∏—Å–∞–Ω–∏–µ | –ò–∫–æ–Ω–∫–∞ |
|-------------|----------|--------|
| **–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Ç–µ–æ—Ä–∏—è** | –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏ | üìö |
| **–†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –Ω—É–ª—è** | –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ ML | üõ†Ô∏è |
| **–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö** | –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ | üåç |
| **–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è** | –û–±—à–∏—Ä–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è | üìä |
| **–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π** | –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ | ‚úÖ |

### üìà –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- **–ü–æ—à–∞–≥–æ–≤—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã** —Å —Ñ–æ—Ä–º—É–ª–∞–º–∏ LaTeX
- **–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞** —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
- **–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑** —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è** –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- **–ü–æ–¥—Ö–æ–¥—ã –∫ —Ä–µ—à–µ–Ω–∏—é** —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º ML
- **–í–∏–∑—É–∞–ª—å–Ω—ã–µ –ø–æ–º–æ—â–Ω–∏–∫–∏** –∏ –¥–∏–∞–≥—Ä–∞–º–º—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

## üìì –û–±–∑–æ—Ä –Ω–æ—É—Ç–±—É–∫–æ–≤

### 1. üìà –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
<div align="center">

| –¢–µ–º–∞ | –û–ø–∏—Å–∞–Ω–∏–µ | –ù–∞–≤—ã–∫–∏ |
|------|----------|--------|
| **–¢–µ–æ—Ä–∏—è** | –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã | –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ |
| **–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ** | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è | NumPy, Matplotlib |
| **–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏** | –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞ | scikit-learn |
| **–†–µ–∞–ª—å–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ** | –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ –∂–∏–ª—å–µ –≤ –ë–æ—Å—Ç–æ–Ω–µ | Pandas, –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö |
| **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è** | –ú–µ—Ç–æ–¥—ã Ridge –∏ Lasso | –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π |
</div>

### 2. üéØ –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
- **–¢–µ–æ—Ä–∏—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏** —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
- **–ö–µ–π—Å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–π –∫–∞–º–ø–∞–Ω–∏–∏**
- **–¢–µ—Ö–Ω–∏–∫–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö** –∏ one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
- **–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏** —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é, F1-score –∏ ROC-AUC
- **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å SVM** –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

### 3. üß† –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞
- **–ê–ª–≥–æ—Ä–∏—Ç–º –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞** —Å –ø–µ—Ä–≤—ã—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤
- **–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞** —Å –ø—Ä—è–º—ã–º/–æ–±—Ä–∞—Ç–Ω—ã–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ–º
- **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö** –≤–∫–ª—é—á–∞—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏ —Ä–µ–∞–ª—å–Ω—ã–µ
- **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** —Å –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–æ–º –∏–∑ scikit-learn
- **–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ø–æ–ª–∞** –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º –≥–æ–ª–æ—Å–∞

### 4. üîÆ –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏
- **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏** —Å —Å–∏–≥–º–æ–∏–¥–∞–ª—å–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π
- **–¢–µ—Ö–Ω–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º**
- **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å LogLoss** –∏ –∞–Ω–∞–ª–∏–∑
- **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å** (MSE vs LogLoss)
- **–ü—Ä–æ–±–ª–µ–º–∞ –∑–∞—Ç—É—Ö–∞—é—â–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤** –∏ —Ä–µ—à–µ–Ω–∏—è

## üí° –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### üìà –ü—Ä–∏–º–µ—Ä –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = LinearRegression()
model.fit(X_train, y_train)

# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
predictions = model.predict(X_test)

# –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
mse = mean_squared_error(y_test, predictions)
print(f'Test MSE: {mse:.4f}')

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
plt.figure(figsize=(10, 6))
plt.scatter(y_test, predictions, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è')
plt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
plt.title('–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è: –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ')
plt.show()
```

### üéØ –ü—Ä–∏–º–µ—Ä –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model_logistic = LogisticRegression(random_state=42)
model_logistic.fit(X_train, y_train)

# –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
y_pred_proba = model_logistic.predict_proba(X_test)[:, 1]
y_pred = model_logistic.predict(X_test)

# –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
print("Classification Report:")
print(classification_report(y_test, y_pred))
print(f"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")
```

### üß† –ü—Ä–∏–º–µ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞
```python
from sklearn.metrics import accuracy_score

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞
perceptron = Perceptron()
losses = perceptron.fit(X_train, y_train, num_epochs=300)

# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
predictions = perceptron.forward_pass(X_test)

# –†–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏
accuracy = accuracy_score(y_test, predictions)
print(f'–¢–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞: {accuracy:.4f}')

# –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
plt.plot(losses)
plt.title('–ü–æ—Ç–µ—Ä–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞')
plt.xlabel('–≠–ø–æ—Ö–∞')
plt.ylabel('–ü–æ—Ç–µ—Ä–∏')
plt.show()
```

### üîÆ –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å LogLoss
```python
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å LogLoss
neuron = Neuron()
J_values = neuron.fit(X, y, num_epochs=5000)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
predictions = neuron.forward_pass(X_test)
binary_predictions = (predictions > 0.5).astype(int)

# –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
accuracy = accuracy_score(y_test, binary_predictions)
print(f'–¢–æ—á–Ω–æ—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏: {accuracy:.4f}')

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—è
plt.figure(figsize=(10, 6))
plt.plot(J_values)
plt.title('–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ (LogLoss)')
plt.xlabel('–ò—Ç–µ—Ä–∞—Ü–∏—è')
plt.ylabel('LogLoss')
plt.grid(True)
plt.show()
```
